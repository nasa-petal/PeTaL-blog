<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[PeTaL Blog]]></title><description><![CDATA[Latest news and updates from the PeTaL dev team.]]></description><link>https://nasa-petal.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 04 May 2021 18:27:50 GMT</lastBuildDate><item><title><![CDATA[Engineering Elegance from Nature: An Interactive Search Tool for Biomimetics]]></title><description><![CDATA[Biologically inspired design derived from the process of learning from nature has created an entire field that aims to integrate design approaches influenced by nature and living systems. From this stems biomimetics, an interdisciplinary design method that strives to emulate how organisms tackle fundamental problems in engineering. Bridging the gap between biological systems and human technology has incentivized a bottom-up approach for biologically inspired design that focuses on the principles of biomimicry and biomimetics. ]]></description><link>https://nasa-petal.github.io/posts/engineering-elegance-from-nature</link><guid isPermaLink="false">https://nasa-petal.github.io/posts/engineering-elegance-from-nature</guid><pubDate>Thu, 29 Apr 2021 23:46:37 GMT</pubDate><content:encoded>&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/3189f19dfd0b350c6564cd26eca377a8/a42c7/jungle.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 35.416666666666664%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAEAA//aAAwDAQACEAMQAAABzLDKgA//xAAZEAACAwEAAAAAAAAAAAAAAAABAgADIRL/2gAIAQEAAQUCSzgl9Gz/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPwFJ/8QAFxAAAwEAAAAAAAAAAAAAAAAAAAEQcf/aAAgBAQAGPwIWz//EABcQAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQEAAT8hVeDsLFgwqP/aAAwDAQACAAMAAAAQj8//xAAWEQEBAQAAAAAAAAAAAAAAAAAAAWH/2gAIAQMBAT8Qxa//xAAXEQEBAQEAAAAAAAAAAAAAAAABABEh/9oACAECAQE/EAOy43//xAAaEAEAAwADAAAAAAAAAAAAAAABACFBMVGB/9oACAEBAAE/ECrV+1EC0xTgjkfCB0rP/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/3189f19dfd0b350c6564cd26eca377a8/8ac56/jungle.webp 240w,
/blog/static/3189f19dfd0b350c6564cd26eca377a8/d3be9/jungle.webp 480w,
/blog/static/3189f19dfd0b350c6564cd26eca377a8/e46b2/jungle.webp 960w,
/blog/static/3189f19dfd0b350c6564cd26eca377a8/879a2/jungle.webp 1263w&quot;
              sizes=&quot;(max-width: 960px) 100vw, 960px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/3189f19dfd0b350c6564cd26eca377a8/09b79/jungle.jpg 240w,
/blog/static/3189f19dfd0b350c6564cd26eca377a8/7cc5e/jungle.jpg 480w,
/blog/static/3189f19dfd0b350c6564cd26eca377a8/6a068/jungle.jpg 960w,
/blog/static/3189f19dfd0b350c6564cd26eca377a8/a42c7/jungle.jpg 1263w&quot;
            sizes=&quot;(max-width: 960px) 100vw, 960px&quot;
            type=&quot;image/jpeg&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/3189f19dfd0b350c6564cd26eca377a8/6a068/jungle.jpg&quot;
            alt=&quot;Lush jungle&quot;
            title=&quot;Lush jungle&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Biologically inspired design derived from the process of learning from nature has created an entire field that aims to integrate design approaches influenced by nature and living systems. From this stems biomimetics, an interdisciplinary design method that strives to emulate how organisms tackle fundamental problems in engineering. Bridging the gap between biological systems and human technology has incentivized a bottom-up approach for biologically inspired design that focuses on the principles of biomimicry and biomimetics. &lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://www.mdpi.com/2411-9660/3/3/43/htm&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Periodic Table of Life (PeTaL)&lt;/a&gt; is an open-source, artificial intelligence (AI) tool that aids in the design and integration of aerospace and human systems based upon biomimetic and bionic processes. It aims to streamline the process between steps in the biologically inspired design process and accessibly present solutions to biologists and engineers through relevant biological literature and data relating to functional living systems. &lt;a href=&quot;https://www1.grc.nasa.gov/research-and-engineering/vine/petal/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;PeTaL&lt;/a&gt;  specifically leverages the latest advancements in machine learning and natural language processing (NLP) to develop an AI-powered system that can learn and suggest solutions to biomimetic design inquiries. It is built for engineers and designers who intend to solve their structural problems with nature’s solutions and for biologists looking to apply their research to engineering.​​​​​​​&lt;/p&gt;
&lt;p&gt;Recently, a goal was set to build a new minimum viable product (MVP) application, based on a dataset of over 1,400 peer-reviewed and labeled biological journal articles and a redesign of the existing web application, Biology Inspired Nature Design, or BIRD. Through a specialized search engine, BIRD “translates” common engineering challenges to biological terminology. A user-friendly PeTaL web interface will let users interact with an NLP-based labeling tool that classifies papers by relevant biological functions and a dynamic catalogue of those scientific papers. A RESTful (representational state transfer) API is also being integrated for internal and external development use.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 833px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/758cd5fa658276fe6851322aba3e0e23/5205c/architecture.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 85.83333333333334%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAACHVAAAh1QEEnLSdAAAByElEQVQ4y4VT226DMAzl//+rmlRpD5tWrWK0MKDlEpJwSQjQkO5AOlaxTvMDGCfHPj42Tt/3TdPgqbW+XC55np9OJzjjbMYYPPW3DcNwvTMH9/ACmHPOGEvTlBACACJt2yql7gH28gNwkiRlWRZFUVUV52UUhlEUIYik/4BRSgjRdR18UEU1KSUy2kuI/AlezoRU/TB9grBtcjn6H6z1OAyTVKBg+cdxTCmt69p2/gCMCjdtZ5jVKQgCwA6HA9qGkADj5lptpEcRiIxL9WxZlqFb5Lr+slUXDmCu64ZhCAzURlnIu9CDg0RL2XVlMNztdsB7nodEwCORXQ8cIxeoIUgLQimTqsMkFlIOjjebzXa7PR6P6BMDm9ZL/3A2oxn6AcngtVnQpknwGWOhJjBYQRWUhbAAQxtE3eOOMQIn9wNKJqeKo/2WndyPgikhO2w0qDlWarvY1vAZnpNP95kz2jaKxbH3WhFvf44bLkxVK3C06+RYDe9lnPGYmhFS95xm7y+R34jedP30Y+BPwVx837+BV9OAuGbUMmVvT2VFWE5qVkpExvGySGWLOddHZrTm5zRJaVkLu3Dzqq6n/RhsDVoq1d7v78q+AOuE2AbOU6roAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/758cd5fa658276fe6851322aba3e0e23/8ac56/architecture.webp 240w,
/blog/static/758cd5fa658276fe6851322aba3e0e23/d3be9/architecture.webp 480w,
/blog/static/758cd5fa658276fe6851322aba3e0e23/184c4/architecture.webp 833w&quot;
              sizes=&quot;(max-width: 833px) 100vw, 833px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/758cd5fa658276fe6851322aba3e0e23/8ff5a/architecture.png 240w,
/blog/static/758cd5fa658276fe6851322aba3e0e23/e85cb/architecture.png 480w,
/blog/static/758cd5fa658276fe6851322aba3e0e23/5205c/architecture.png 833w&quot;
            sizes=&quot;(max-width: 833px) 100vw, 833px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/758cd5fa658276fe6851322aba3e0e23/5205c/architecture.png&quot;
            alt=&quot;Figure 1: The planned architecture for PeTaL, and the cloud infrastructure workflow for the labeler&quot;
            title=&quot;Figure 1: The planned architecture for PeTaL, and the cloud infrastructure workflow for the labeler&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
Figure 1: The planned architecture for PeTaL, and the cloud infrastructure workflow for the labeler&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 738px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/14d06dd6096022172d2f476a880149c2/774b6/sw-arch.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 85%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACmUlEQVQ4y2VT2XKbQBDU//9LXpyHpJJyykI5bCfilEAIcbPcQtzscikDWCqn0lALLPRMz0yzut4wjiOsuqY+PHykGZaiqG+PjxzPcRz7+/WVoja/np95gatJRfqGdA18v8IYl0VR5EVd123bns9nwzAQQr7vJ0kSBEEYBP6EAL6RHUF2eTNUDog9l/4qzzNlRhiGVVVFUajIiriX4vgMj2maep7nIhfid22nhZIb2XmWW4FmhMcVZOtmNE0zDAMmuIEDY0JI3/ew4jc04zBGuXf0BSNSzET2L2gFdQKHkDZLU1BOIFbbzvSJAIG6GyBN3w1wA0GBBa9XUDdIFffifici5C7Nu1wu+91+J+x9L2iaur+h6yfmXelEhjRRCH0JyrJcel5WpYMc27azPIMM7TtAIcsKUSbyLJtUdQ13wCS49fTI1UKkBp4W4ZrMk7z+j4nsODbPctstrWvGeB3ypFIE3dRNy7RkXnUNvx8mke3UPLjWsyneyAPUrKuapuqBH3R9WxfYkJB1QrbqnXgzjfMlT9F0ql/vzNyKGkz6hTwp6jsYUX2z2aQc1y2pW1wRmBDsVLjbO40cDKf4evCHo9v0w7C6jv3ZebGPa+PwFJrfe5KP9xjvbGt4xZo2v/48fKKEpz/qhkNxRoCMLXkt8BIvHCx1W2cmSJpruwMMgwXF3zD2lx/ih88v661OMRaKSpDdFxGX+uzF44pg2+Hk+m9zFxHnHNNqISE8n4RW06ohk8Pgt0COaRpaeknuOl3XFUXxeDxKkgS/Cux4aSu6rWBP/KTo3kYF/qC3DMNwi8MWMseyLMMcDjLP8dRmMww9DCrNSz/O2m6cMZOhrDiOoyiuqvpOPp1ONEMfFYUXBAgMO2DJxUX3Wv4CB0G13hBiHUEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/14d06dd6096022172d2f476a880149c2/8ac56/sw-arch.webp 240w,
/blog/static/14d06dd6096022172d2f476a880149c2/d3be9/sw-arch.webp 480w,
/blog/static/14d06dd6096022172d2f476a880149c2/65afa/sw-arch.webp 738w&quot;
              sizes=&quot;(max-width: 738px) 100vw, 738px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/14d06dd6096022172d2f476a880149c2/8ff5a/sw-arch.png 240w,
/blog/static/14d06dd6096022172d2f476a880149c2/e85cb/sw-arch.png 480w,
/blog/static/14d06dd6096022172d2f476a880149c2/774b6/sw-arch.png 738w&quot;
            sizes=&quot;(max-width: 738px) 100vw, 738px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/14d06dd6096022172d2f476a880149c2/774b6/sw-arch.png&quot;
            alt=&quot;Figure 2: The earliest architecture of BIRD, developed by 2019 PeTaL intern Lauren Friend&quot;
            title=&quot;Figure 2: The earliest architecture of BIRD, developed by 2019 PeTaL intern Lauren Friend&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
Figure 2: The earliest architecture of BIRD, developed by 2019 PeTaL intern Lauren Friend&lt;/p&gt;
&lt;p&gt;The article labeling tool uses text classification and NLP models to generate function labels based on a unique biological taxonomy curated by the PeTaL development team.  Our final model will be used to classify thousands of relevant publications and label them with one or more biological functions catered to the end user’s searches. Each function label has corresponding subgroup functions, split up into Level I, II, and III functions. For example, a paper about how penguins retain their heat could be classified with the “Protect from temperature” Level III function label. Therefore, it would also fall under the “Protect from non-living threats” Level II function label, and “Protect from harm” Level I function label. (see Fig. 3) For the MVP, publications will be labeled with Level I function labels from the PeTaL taxonomy. Once papers are classified, the PeTaL interface will allow users to search for functions that solve specific engineering problems.&lt;/p&gt;
&lt;p&gt;The model driving our labeling tool is built using &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Transformers&lt;/a&gt;, a unique bidirectional neural network that is able to solve large sequence-to-sequence NLP problems. Transformers rely on self-attention, which allows models to better derive context and semantic meaning of words and phrases through the way it computes similarity. As the transformer model processes each sentence, the self-attention layer mechanisms operate to have each word analyze its surrounding words to see how a specific word relates to the rest of the sentence and its contextual meaning.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 757px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/bfa84e84c53d1f4bea390c9d6b042172/1fbe8/text-classification.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 50.416666666666664%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABqUlEQVQoz31Su27CQBDkw/L4HdKQf6ClpQ2KEA1dKqhSoMSQpAmByBYST/EWAWxj/MA+T/bW2AlEykor3+2t52ZnLtXpdNDr9aBpGtrtNlqtFprNJgaDAbrdLlRVhe/7kBGG4UnKsG0bpVIJiqLwPuW6Lhcdx4FpmrAsi7+yJs9Mw4QIBEIRJY5AQq4plsslcrkcarVaBIh/gnj8qYmY4ZHddDqFUq8TkX0EGFM/AaKaCAUcw8LkTcP4RcXkVeW1re+SviAI5BVyRR+fWSeAjUYDxWKRx41jTCCF61vkL9LIX96gcJXB8LlFv4dwbIek8LHZHfA53GHy5bLWPLLUr1wuI5vNYjQawZO6+h6zuiMQCZa/TBN4Br3aO9Y7HYv5AnvLQH95wP3jGo2Oy5MxoOd5qFarqFQqiaNSK9fc87j9pw9mNlLaZyMLGjMgYBOGvmVDU7Fms9kMq9Uq0TDwg8SUjanTaAa2lAG7/eO01HGzofOtzq8i0XA+n7NjiZvULNn6hwODx3n6JrkTrq2Tnu6py/KmyDXg3Pnzx/y7RzgzrPsPcNZN3n8DCn7vrLiiN+wAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/bfa84e84c53d1f4bea390c9d6b042172/8ac56/text-classification.webp 240w,
/blog/static/bfa84e84c53d1f4bea390c9d6b042172/d3be9/text-classification.webp 480w,
/blog/static/bfa84e84c53d1f4bea390c9d6b042172/4578a/text-classification.webp 757w&quot;
              sizes=&quot;(max-width: 757px) 100vw, 757px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/bfa84e84c53d1f4bea390c9d6b042172/8ff5a/text-classification.png 240w,
/blog/static/bfa84e84c53d1f4bea390c9d6b042172/e85cb/text-classification.png 480w,
/blog/static/bfa84e84c53d1f4bea390c9d6b042172/1fbe8/text-classification.png 757w&quot;
            sizes=&quot;(max-width: 757px) 100vw, 757px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/bfa84e84c53d1f4bea390c9d6b042172/1fbe8/text-classification.png&quot;
            alt=&quot;How a simple text classification process with a machine learning model works (Image from katacoda.com)&quot;
            title=&quot;How a simple text classification process with a machine learning model works (Image from katacoda.com)&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
How a simple text classification process with a machine learning model works (Image from katacoda.com)&lt;/p&gt;
&lt;p&gt;Let’s look at a specific example: in the sentence “The dog jumped over the fence because it was being chased”, what does the word “it” refer to? By analyzing the sentence’s subject and predicate structure, we can derive that “it” is referring to the “dog”. If we were to keep adding more phrases to this sentence, it could get more complicated and difficult to understand the underlying context. However, with the Transformer’s ability to “focus” on certain areas of a sentence through attention mechanisms, it is able to understand context of all surrounding words. ​​​​​​​&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 534px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/1fd7a112edff979496179f37e0b73dfd/a07a7/taxonomy.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 136.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAbCAYAAAB836/YAAAACXBIWXMAACHVAAAh1QEEnLSdAAAFFklEQVRIx3VVa2hbZRgOiOC/4T8VBX/4w+nQH6KIoGN/xCuCG24Igqsw6Cx0PyaooDC8UAQRCsKmTBCKgtKua3qxXYu9LU3SLpemuScnJ/fLOUlOTpK2a7s+vu+bJourBj6+k+Q7z/d8z/O872cBfQpaGYlMHnm9Ct0wUTabMJrbqDS2UKnVaTRQpWeDRrW+2fq93kSNRoW+8388avSOhQGzhRIiShLBqIJSpUZ/bqJUNWURb1asGLJBkf5jgApvWG90wIwDQJ4te3t7iKsphAkwpqaRK5VRIKa8QVGvIJpIQiNwZp8rVeglAjEPg/F3o0mAjUYD/mAYUUVFIpVFKpsnkJQAJlIZ2YDZxpIZOa5OzzpJwC+3gVpz67sF+/uIZkpYjWQQVHNIFnRhFYkrUGgDzajLBjrNWrWGTFHvaFztYsejRuxFQ69awog9jOm1MKbWIrD5FcTTeRTLVURJBtaQn/MkAZvBYMySNWXwKv3GMhhtQDsBDC+tY8K+gZHldVhvejHjVrCu5MSUvFYRsGS2gGAsIcwVkUMngxqySfvolr3dHcw5PJhxRTHpDODaohvjNh/+XFzH9K0IolkNuaKGACXA6w8hRIBJ0pnjVT046qHYDFkX8Ong7xj6y4YZp1/YThH4rDuGSVccfiWD9UBImLFJbUO6h7gsptDnwsAvsBx9By+evojzX/+MaQIbX9nAmCMMqzMMRygFNZ0jdoqY0jaEgfm5PToMe78cxIMvvI8nX+/Fwy9/KKCTziCstg3MkFHT7jjCyTySmZzo2A49J4ANK5RbQ3LIgFPzq3ivfwAvnfkET73Vh0eO9+Dq9QU6dhDDix5h6UsUJJdxyqNKwMxYNqDBGufJIL1qwLK7twO9mcTWdhMfX7qMx058hCPEtverK5h2bODaggvWFT9W/AlEYnEEIjHKqCojSuFng5glJ4ErrJXDQATblPTfxufx+IkePHr8LE5e+I7cpig5Q5giY1ZCaaSITZyMUQlEpaOrGc6qIYZwHmubBxr+8asbf1v96B+4ggeePYUjz5/BeWI4sxbEKOVyZNkHWyCJTL4IhY6aKWjChhsJzzmtVf8CuH/nDsZmHTj3xY947t1+PPTKWdz/zCn8MDSBCZsHo4sujNv9cPrjCNFx/eGolCLnMUJzOl+STXguUDVZVDWJ13o+F+2Ovd2HJ149h9MXvycwL0aXvBhbCcDqCEItlEX8NLFU09xECqIfdyoGSxNr7koWr8+Pp9/sxX3HTuLoG7344LNBLLhCuL7kxhjFZnItiqVASha3I8JxKZsN6ZtlasBlbspcOWYdlkAogr5vfkLft1dxeXgOs6Qb1/OiN4YbnrgYouTL0hsZjLuOcRDqVri3uyqGmkMoGsfEshtzrjAmV0MYW/aQs0FMELMFAg2mNcpXrWOEmEFsjf8qPa4Uh9uHkfk1Ep+OSHkbvenDjVthjK/F4IlnUTNN6SYcDwbjuQN07xXAlcLHGbMHpCq4dkfJBNuGgmzZFAPYTdamzaC7MXS3/063KRpN2Kn4ufcF0jpylbpcQhXDlIUMmMoVYZAJ0ky79OsG7nSb7Z1dusFoIblV37oNc7MlssY3nNlqnCmKhUYbGGZrszbIvTqKhg0C4Qiw8P8SmV7Uqq3rk5lpfDfzXfw/oJ1bz9y8LQt0aUnbdxfQkHwR83Y0+Gbjlm/W7x7/kCksJLsoLf1eseml8oGWxkHO5Lqk9Waj2WHafQX8A7zkiPGVXh3UAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/1fd7a112edff979496179f37e0b73dfd/8ac56/taxonomy.webp 240w,
/blog/static/1fd7a112edff979496179f37e0b73dfd/d3be9/taxonomy.webp 480w,
/blog/static/1fd7a112edff979496179f37e0b73dfd/29722/taxonomy.webp 534w&quot;
              sizes=&quot;(max-width: 534px) 100vw, 534px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/1fd7a112edff979496179f37e0b73dfd/8ff5a/taxonomy.png 240w,
/blog/static/1fd7a112edff979496179f37e0b73dfd/e85cb/taxonomy.png 480w,
/blog/static/1fd7a112edff979496179f37e0b73dfd/a07a7/taxonomy.png 534w&quot;
            sizes=&quot;(max-width: 534px) 100vw, 534px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/1fd7a112edff979496179f37e0b73dfd/a07a7/taxonomy.png&quot;
            alt=&quot;The PeTaL Biomimicry Taxonomy, with Level I, II, and II subgroups&quot;
            title=&quot;The PeTaL Biomimicry Taxonomy, with Level I, II, and II subgroups&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
The PeTaL Biomimicry Taxonomy, with Level I, II, and II subgroups&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/bf60f847e4dc1478a64a7bb85b04e640/29007/ml.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 57.50000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAACGElEQVQoz3VS23LaMBT0/39BX9rHvqQZHloSCIHWGIITcJwAvgQDBkwSwPZgiLnbbKVDzcC0PWNZ0pG0Wu0eYb/fA/zjPYvVcoU4jg/j1QqqqkKWZVSrVVQqFbTb7ePepD8N4XRhHn5AvJPgvA5pvtlsEAQBbLsPw2zB933MZjPa/78m8F/CyHVdZDJp6LpG8+12S73nDdBqKUcWcfw3szOGSVjdDi4YoChXMJ/P6cZ+7x2SWEYum0P92cJk4lHe83zWvD/NJzKLxQJCyJ75OuzB6XWgmQa+52+Ql0QE04AdjFGSVFxeFFjLI3VZgGV1SNuaopCuCutrtRrp7DgOBN0wkbrN4tv1FWpyEaX0Z8i/fmA88Ym1+qSiWCpCLIqQyhLTdIrdbnd8laI0oT7qNOZ5wWKuSVUFD88NNBtP+JlP4a5cYAZMSVv+9PV6TWaEYYgoisgsHi+tF6QZEdvukQxcc8E0NJQzXyBlv6KlKZCvPqEuZxHMQ+4/HeaAy+WSxvxQYpZtmpBz1/Am7tFEodPu4L5Sgt6sYzR6h1JlNfdwT4ySkkpAk9JIALtdG7mbDJnCLyfAwWCAxqMKfzzBlh20NB1GU2Pgo+PTkkjqNQEcj1wGOsSUDDxcLOzYYrvegGNZWDC9DOaY+/Z2BnBauIc6jMmAOI6Y4wcpuLZkCt+wZGWwZZMdS/aZ9R9heAb0L5ZJRFF8lvsNjgSDGHfOetYAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/bf60f847e4dc1478a64a7bb85b04e640/8ac56/ml.webp 240w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/d3be9/ml.webp 480w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/e46b2/ml.webp 960w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/f992d/ml.webp 1440w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/fad48/ml.webp 1600w&quot;
              sizes=&quot;(max-width: 960px) 100vw, 960px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/bf60f847e4dc1478a64a7bb85b04e640/8ff5a/ml.png 240w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/e85cb/ml.png 480w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/d9199/ml.png 960w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/07a9c/ml.png 1440w,
/blog/static/bf60f847e4dc1478a64a7bb85b04e640/29007/ml.png 1600w&quot;
            sizes=&quot;(max-width: 960px) 100vw, 960px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/bf60f847e4dc1478a64a7bb85b04e640/d9199/ml.png&quot;
            alt=&quot;Figure 5: Transformers architecture, with an emphasis on the attention layers that are amplified through a scaled dot-product process, repeated in parallel with multiple heads (Image from towardsdatascience.com)&quot;
            title=&quot;Figure 5: Transformers architecture, with an emphasis on the attention layers that are amplified through a scaled dot-product process, repeated in parallel with multiple heads (Image from towardsdatascience.com)&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
Figure 5: Transformers architecture, with an emphasis on the attention layers that are amplified through a scaled dot-product process, repeated in parallel with multiple heads (Image from towardsdatascience.com)&lt;/p&gt;
&lt;p&gt;Through transfer learning, where a trained model or neural network can be applied to another task or domain, we created a Transformers-based labeler model fine-tuned using the &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;BERT model&lt;/a&gt; architecture. BERT specifically allows for better understanding of contextual meaning within the journal articles, since it uses a masked language model through semi-supervised sequence learning to derive context and semantics from surrounding words.&lt;/p&gt;
&lt;p&gt;BERT uses masking to hide a few words in a phrase or sentence and forces the model to predict the masked words based on the surrounding, non-masked words within the sequence; comparable to a fill-in-the-blank process. This process means the BERT model cannot “cheat” by looking at the following word, thereby learning nothing. Following this, BERT uses another training technique, called next sentence prediction, to predict an entire sentence just by using its preceding sentence. Pretrained on a large corpus of unlabeled text consisting of over 3.2 billion words from BookCorpus and Wikipedia combined, BERT reduced our project’s dependence on a larger training dataset. For further multi-label text classification, the pretrained BERT model was altered specifically for this task with the use of an additional sequence classification layer.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 876px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/blog/static/3a690a2ea178f550028748fdbb5c3996/1b1d5/transformer.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 67.91666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACNElEQVQoz12SSY+bQBCF/f+POeeWa6Rc55DbHKIoyziaGTtgA73iZmvWphcaSGEkH4Ke0KdXr6DU1Yd1XYdBEcIIpgBK6ShKGEv1qKtSXoJLUUrddfTzp9PHD/XPb0rbJEYYEQgcnp6ewksgCkR4oJ0EAeQVBujHHJGzbFNrK/HyNX7+0rM37Rt+uybkZJ0+hEFQNl1r7U1Wx/P5eD79jWPQKYrer1eA90v4GgYXIU6UBYy9hiEkpR576w4wtp2WRtmy7kVRg2iac1HmssvKBjjNZFa1t1wSJiBTyE62quo0dG3NxhhGKSEYwE9TksRFnjtrh6GPo6iW0s++qkrwjdbGaMYoShJrzdY8TVMmMs7TrWQsxiTPcqMNnB9CWMraWldVEiM8jhoyQmSMMujamrVfqJ6omVI3g6h23PrbtAi/srsJDA74AHsGj1b5+9i19Ymf2brSu3aInYutTSa36+GDOFT9XFq/NTfWYz+nd3cXW5bfnH9HCbx/MfoDrsGyPKqQhJ/JvVkaf50WtKzJvOkB//FDeFnDaSmNP9R13SpFhzYde2FVZkfS16nub2bI3Eg64AF8YRRTbe40MFRRJ808H+BseQqLOqU8UIMYVYaS11sa6jGva5pEf4osdraKrsfjy3NbUwgwesboTet+G9s5hxARothWZe8MqzK274c4RmUpvZ8J5ee/Qd8rbWyaCowp7O8wz/N2Zk0D86/3J8+zrut25pzDp/eLJITYTQgXRQHwD05FDFRYgG2iAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/blog/static/3a690a2ea178f550028748fdbb5c3996/8ac56/transformer.webp 240w,
/blog/static/3a690a2ea178f550028748fdbb5c3996/d3be9/transformer.webp 480w,
/blog/static/3a690a2ea178f550028748fdbb5c3996/21dbd/transformer.webp 876w&quot;
              sizes=&quot;(max-width: 876px) 100vw, 876px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/blog/static/3a690a2ea178f550028748fdbb5c3996/8ff5a/transformer.png 240w,
/blog/static/3a690a2ea178f550028748fdbb5c3996/e85cb/transformer.png 480w,
/blog/static/3a690a2ea178f550028748fdbb5c3996/1b1d5/transformer.png 876w&quot;
            sizes=&quot;(max-width: 876px) 100vw, 876px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/blog/static/3a690a2ea178f550028748fdbb5c3996/1b1d5/transformer.png&quot;
            alt=&quot;Figure 6: Transformer network layers and embeddings with the added classification layer (Image from towardsdatascience.com)&quot;
            title=&quot;Figure 6: Transformer network layers and embeddings with the added classification layer (Image from towardsdatascience.com)&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
Figure 6: Transformer network layers and embeddings with the added classification layer (Image from towardsdatascience.com)&lt;/p&gt;
&lt;p&gt;An important piece of Transformers, BERT, and other NLP models is the tokenizer. A tokenizer uses lexical analysis to convert a stream of text into token sequences. This process is necessary to develop semantic units for processing into a pattern that a machine or complier can understand for processing. It splits sentences and phrases into smaller units which are then matched up to distinguishing numerical identifiers. BERT tackles tokenization through the &lt;a href=&quot;https://arxiv.org/pdf/1609.08144.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;WordPiece algorithm&lt;/a&gt;, by using sub-words to break down word structures into building blocks for other terms within a corpus (i.e. burning = bu + rn + ing –&gt; tu + rn + ing = turning).&lt;/p&gt;
&lt;p&gt;Common words like those listed above can be easily broken down and built back up. However, more complex terms, like scientific and biological words, are usually not present in a model’s default token library. This poses an issue for the PeTaL labeler, because the papers used to train our model with are filled with scientific terminology.&lt;/p&gt;
&lt;p&gt;Initially, we added our own vocabulary into BERT’s tokenizer model, but this modified the embeddings on which BERT was originally trained and risked a reduction of its performance and accuracy. After thorough research, we came across &lt;a href=&quot;https://www.aclweb.org/anthology/D19-1371/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;SciBERT&lt;/a&gt;, a BERT model trained on scientific text, developed by the Allen Institute of AI. This model has its own vocabulary for the tokenizer catered to match the training corpus of Semantic Scholar papers. We are in the process of refining our model to the SciBERT architecture and vocabulary through transfer learning.&lt;/p&gt;
&lt;p&gt;One of the biggest challenges in developing the labeling tool has been the lack of annotated training data, resulting in low accuracy rates. Our current methods include scraping multiple journal servers for articles and abstracts that have been prelabeled by our team of biologists. We are also using Amazon Mechanical Turk to crowdsource labeled data from biomimicry and biology professionals across the world. &lt;/p&gt;
&lt;p&gt;Through our development of a cloud infrastructure, the labeling model will be implemented within the PeTaL architecture through AWS Sagemaker and the PeTaL API. This will allow the model to streamline communication between the web interface and dynamic database of publications.&lt;/p&gt;
&lt;p&gt;The PeTaL team is continuing to develop an MVP with a growing team of NASA engineers, interns, biologists, and other open-source contributors, as we envision a streamlined product that makes biologically inspired design tools accessible and understandable to a wider scientific audience beyond experts in biomimicry. We are seeking engineers and researchers who may be potential users of PeTaL to test our product in the coming months. Please &lt;a href=&quot;https://www1.grc.nasa.gov/research-and-engineering/vine/contact-us/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;reach out to us&lt;/a&gt; if you are interested.&lt;/p&gt;
&lt;p&gt;— Shruti Janardhanan&lt;/p&gt;
&lt;h3 id=&quot;references&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#references&quot; aria-label=&quot;references permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h3&gt;
&lt;p&gt;​​​​​​​​​​​​​​Beltagy, I., Lo, K., &amp;#x26; Cohan, A. (2019). SciBERT: A Pretrained Language Model for Scientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 3615–3620. &lt;a href=&quot;https://doi.org/10.18653/v1/D19-1371&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://doi.org/10.18653/v1/D19-1371&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Devlin, J., Chang, M.-W., Lee, K., &amp;#x26; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv:1810.04805 [Cs]. &lt;a href=&quot;http://arxiv.org/abs/1810.04805&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;http://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ghati, G. (2020, May 13). Head Pruning in Transformer models ! Medium. &lt;a href=&quot;https://towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Horev, R. (2018, November 17). BERT Explained: State of the art language model for NLP. Medium. &lt;a href=&quot;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PeTaL. (n.d.). Glenn Research Center | NASA. Retrieved April 29, 2021, from &lt;a href=&quot;https://www1.grc.nasa.gov/research-and-engineering/vine/petal/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://www1.grc.nasa.gov/research-and-engineering/vine/petal/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Shyam, V., Friend, L., Whiteaker, B., Bense, N., Dowdall, J., Boktor, B., Johny, M., Reyes, I., Naser, A., Sakhamuri, N., Kravets, V., Calvin, A., Gabus, K., Goodman, D., Schilling, H., Robinson, C., Reid II, R. O., &amp;#x26; Unsworth, C. (2019). PeTaL (Periodic Table of Life) and Physiomimetics. Designs, 3(3), 43. &lt;a href=&quot;https://doi.org/10.3390/designs3030043&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;https://doi.org/10.3390/designs3030043&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Text Classification | Katacoda. (n.d.). O’Reilly KataKoda. Retrieved April 29, 2021, from /basiafusinska/courses/nlp-with-python/text-classification&lt;/p&gt;
&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp;#x26; Polosukhin, I. (2017). Attention Is All You Need. ArXiv:1706.03762 [Cs]. &lt;a href=&quot;http://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;http://arxiv.org/abs/1706.03762&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. ArXiv:1609.08144 [Cs]. &lt;a href=&quot;http://arxiv.org/abs/1609.08144&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;http://arxiv.org/abs/1609.08144&lt;/a&gt;&lt;/p&gt;</content:encoded></item></channel></rss>